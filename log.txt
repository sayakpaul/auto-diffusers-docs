Downloading 'text_encoder/model-00001-of-00005.safetensors' for inspection...
The `dtype` for component (text_encoder) is torch.float32. Since bf16 computation is not disabled we will slash the total size of this component by 2.
Downloading 'transformer/diffusion_pytorch_model-00001-of-00012.safetensors' for inspection...
The `dtype` for component (transformer) is torch.float32. Since bf16 computation is not disabled we will slash the total size of this component by 2.
Downloading 'vae/diffusion_pytorch_model.safetensors' for inspection...
The `dtype` for component (vae) is torch.float32. Since bf16 computation is not disabled we will slash the total size of this component by 2.

System RAM: 125.54 GB
RAM Category: large

GPU VRAM: 23.99 GB
VRAM Category: medium
current_generate_prompt='\nckpt_id: Wan-AI/Wan2.1-T2V-14B-Diffusers\npipeline_loading_memory_GB: 37.432\navailable_system_ram_GB: 125.54026794433594\navailable_gpu_vram_GB: 23.98828125\nenable_lossy_outputs: False\nis_fp8_supported: True\nenable_torch_compile: True\n'
Sending request to Gemini...
```python
from diffusers import DiffusionPipeline
import torch

ckpt_id = "Wan-AI/Wan2.1-T2V-14B-Diffusers"

# Initialize the pipeline with bfloat16 for better performance and memory
pipe = DiffusionPipeline.from_pretrained(ckpt_id, torch_dtype=torch.bfloat16)

# The model does not fit entirely into GPU VRAM (37.432GB > 23.988GB),
# but ample system RAM is available (125.54GB).
# Therefore, enable model CPU offload to move parts of the model to CPU when not in use.
pipe.enable_model_cpu_offload()

# Enable torch.compile for performance boost.
# Since offloading is applied, use recompile_limit and compile without fullgraph=True.
torch._dynamo.config.recompile_limit = 1000
pipe.transformer.compile()

# Perform inference
prompt = "photo of a dog sitting beside a river"
image = pipe(prompt).images[0]

# You can save the image if needed
# image.save("output_image.png")
```
