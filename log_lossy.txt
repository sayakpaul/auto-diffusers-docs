Downloading 'text_encoder/model-00001-of-00005.safetensors' for inspection...
The `dtype` for component (text_encoder) is torch.float32. Since bf16 computation is not disabled we will slash the total size of this component by 2.
Downloading 'transformer/diffusion_pytorch_model-00001-of-00012.safetensors' for inspection...
The `dtype` for component (transformer) is torch.float32. Since bf16 computation is not disabled we will slash the total size of this component by 2.
Downloading 'vae/diffusion_pytorch_model.safetensors' for inspection...
The `dtype` for component (vae) is torch.float32. Since bf16 computation is not disabled we will slash the total size of this component by 2.

System RAM: 125.54 GB
RAM Category: large

GPU VRAM: 23.99 GB
VRAM Category: medium
current_generate_prompt='\nckpt_id: Wan-AI/Wan2.1-T2V-14B-Diffusers\npipeline_loading_memory_GB: 37.432\navailable_system_ram_GB: 125.54026794433594\navailable_gpu_vram_GB: 23.98828125\nenable_lossy_outputs: True\nis_fp8_supported: True\nenable_torch_compile: True\n'
Sending request to Gemini...
```python
from diffusers import DiffusionPipeline
from diffusers.quantizers import PipelineQuantizationConfig
import torch

# User-provided information
ckpt_id = "Wan-AI/Wan2.1-T2V-14B-Diffusers"
pipeline_loading_memory_GB = 37.432
available_system_ram_GB = 125.54026794433594
available_gpu_vram_GB = 23.98828125
enable_lossy_outputs = True
is_fp8_supported = True
enable_torch_compile = True

# --- Optimization Logic ---

# 1. Quantization (Lossy Outputs + FP8 Support)
# Since lossy outputs are enabled and FP8 is supported, use torchao for FP8 quantization.
quant_config = PipelineQuantizationConfig(
    quant_backend="torchao",
    quant_kwargs={"quant_type": "float8dq_e4m3_row"},
    components_to_quantize=["transformer"]
)

# 2. Load Pipeline
# Use bfloat16 for dtype as a standard practice unless specified otherwise.
pipe = DiffusionPipeline.from_pretrained(ckpt_id, quantization_config=quant_config, torch_dtype=torch.bfloat16)

# 3. Memory Management (VRAM < Pipeline Memory, Large System RAM)
# Pipeline memory (37.432 GB) is greater than available VRAM (23.988 GB).
# System RAM (125.54 GB) is large.
# In this scenario, model offloading to CPU is an effective strategy.
pipe.enable_model_cpu_offload()

# 4. Torch Compile (Enabled)
# Since offloading is applied, use the specific torch.compile() configuration.
torch._dynamo.config.recompile_limit = 1000
torch._dynamo.config.capture_dynamic_output_shape_ops = True
pipe.transformer.compile()

# --- Inference ---
prompt = "photo of a dog sitting beside a river"
image = pipe(prompt).images[0]

# Display or save the image
# image.save("output_image.png")
print("Image generated successfully!")
```
